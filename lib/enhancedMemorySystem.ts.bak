import { GoogleGenerativeAI } from '@google/generative-ai'
import { supabase } from './supabase'

// Enhanced Memory Types (Claude-like)
export interface EnhancedMemory {
  id?: string
  phone_number: string
  memory_type: 'personal' | 'preferences' | 'context' | 'relationships' | 'patterns' | 'experiences' | 'beliefs' | 'professional' | 'physical' | 'temporal'
  memory_key: string
  memory_content: string
  memory_summary?: string
  importance_score: number
  confidence_score: number
  verification_status: 'verified' | 'unverified' | 'disputed' | 'outdated'
  related_memory_ids?: string[]
  semantic_tags?: string[]
  source_conversation_id?: string
  extraction_method: 'ai_automatic' | 'user_provided' | 'inferred' | 'corrected'
  expires_at?: string
  is_active: boolean
}

export interface MemoryCluster {
  id?: string
  phone_number: string
  cluster_name: string
  cluster_type: 'semantic' | 'temporal' | 'contextual'
  description?: string
  memory_ids: string[]
  cluster_score: number
}

export interface ConversationPattern {
  id?: string
  phone_number: string
  pattern_type: 'communication_style' | 'topic_preference' | 'interaction_timing' | 'response_style' | 'help_seeking' | 'feedback_style'
  pattern_description: string
  confidence_level: number
  evidence_count: number
  last_observed: string
}

export interface MemoryVerification {
  id?: string
  memory_id: string
  phone_number: string
  verification_type: 'fact_check' | 'user_confirmation' | 'contradiction_check'
  verification_prompt?: string
  status: 'pending' | 'confirmed' | 'rejected' | 'skipped'
  user_response?: string
}

export class EnhancedMemorySystem {
  private genAI: GoogleGenerativeAI
  private model: any

  constructor() {
    const rawKey = process.env.GOOGLE_AI_API_KEY
    const apiKey = rawKey?.trim().replace(/^['"]|['"]$/g, '') || ''
    
    if (!apiKey) {
      throw new Error('GOOGLE_AI_API_KEY is not configured')
    }
    
    this.genAI = new GoogleGenerativeAI(apiKey)
    this.model = this.genAI.getGenerativeModel({ model: 'gemini-2.5-flash' })
  }

  /**
   * Extract enhanced memories with semantic understanding
   */
  async extractEnhancedMemories(
    userMessage: string,
    agentResponse: string,
    conversationHistory: Array<{ role: string, content: string }> = [],
    conversationId: string
  ): Promise<EnhancedMemory[]> {
    try {
      const systemPrompt = `You are an advanced memory extraction system like Claude's. Extract meaningful information that will enhance future conversations.

ENHANCED MEMORY TYPES:
- personal: core identity (name, age, location, family, identity)
- preferences: likes/dislikes, styles, choices, tastes
- context: ongoing situations, projects, current goals, challenges
- relationships: people, pets, social connections, dynamics
- patterns: communication style, behavioral patterns, routines
- experiences: significant events, stories, achievements, travels
- beliefs: opinions, values, worldviews, philosophies
- professional: work, career, skills, education, ambitions
- physical: health, appearance, body-related, medical
- temporal: schedules, routines, time preferences, availability

ENHANCED FEATURES:
1. Create semantic tags for grouping related concepts
2. Identify relationships between memories
3. Suggest verification for important facts
4. Determine memory expiration (if temporary)
5. Generate human-readable summaries

SCORING GUIDELINES:
importance_score (1-10):
- 9-10: Core identity, major life facts, critical preferences
- 7-8: Important relationships, significant preferences, ongoing situations
- 5-6: Useful details, minor preferences, casual mentions
- 3-4: Temporary states, minor details
- 1-2: Trivial, very temporary

confidence_score (0-1):
- 0.9-1.0: Explicitly stated by user
- 0.7-0.8: Strongly implied or inferred
- 0.5-0.6: Moderately certain
- 0.3-0.4: Weakly inferred
- 0.1-0.2: Speculative

OUTPUT FORMAT:
Return JSON array with enhanced memory objects:
{
  "memory_type": "preferences",
  "memory_key": "food_dietary_restrictions",
  "memory_content": "is vegetarian, has been for 3 years, ethical reasons",
  "memory_summary": "vegetarian for 3 years (ethical)",
  "importance_score": 8,
  "confidence_score": 0.95,
  "verification_status": "unverified",
  "semantic_tags": ["diet", "ethics", "lifestyle", "health"],
  "extraction_method": "ai_automatic",
  "expires_at": null,
  "is_active": true
}

Analyze this conversation and extract enhanced memories:`

      const prompt = `
USER MESSAGE: "${userMessage}"
AGENT RESPONSE: "${agentResponse}"
CONVERSATION ID: "${conversationId}"

RECENT CONTEXT:
${conversationHistory.slice(-4).map(msg => `${msg.role.toUpperCase()}: "${msg.content}"`).join('\n')}

Extract enhanced memories as JSON array:`

      const result = await this.model.generateContent({
        contents: [{ parts: [{ text: prompt }] }],
        systemInstruction: { parts: [{ text: systemPrompt }] },
        generationConfig: {
          maxOutputTokens: 1500,
          temperature: 0.2,
        }
      })

      const response = result.response.text()
      let cleanedResponse = response.trim()
      
      // Clean JSON response
      if (cleanedResponse.startsWith('```json')) {
        cleanedResponse = cleanedResponse.replace(/```json\n?/, '').replace(/```$/, '')
      }
      if (cleanedResponse.startsWith('```')) {
        cleanedResponse = cleanedResponse.replace(/```\n?/, '').replace(/```$/, '')
      }

      const memories = JSON.parse(cleanedResponse) as any[]
      
      // Convert to EnhancedMemory objects
      return memories
        .filter(m => m.memory_key && m.memory_content && m.importance_score >= 4)
        .map(m => ({
          ...m,
          source_conversation_id: conversationId,
          related_memory_ids: m.related_memory_ids || [],
          semantic_tags: m.semantic_tags || []
        }))
      
    } catch (error) {
      console.error('Enhanced memory extraction error:', error)
      return []
    }
  }

  /**
   * Store memories with semantic clustering
   */
  async storeEnhancedMemories(phoneNumber: string, memories: EnhancedMemory[]): Promise<boolean> {
    if (!supabase || memories.length === 0) return false

    try {
      // Store individual memories
      const memoryResults = []
      for (const memory of memories) {
        const { data, error } = await supabase
          .from('user_memories')
          .upsert({
            phone_number: phoneNumber,
            memory_type: memory.memory_type,
            memory_key: memory.memory_key,
            memory_content: memory.memory_content,
            memory_summary: memory.memory_summary,
            importance_score: memory.importance_score,
            confidence_score: memory.confidence_score,
            verification_status: memory.verification_status,
            semantic_tags: memory.semantic_tags,
            source_conversation_id: memory.source_conversation_id,
            extraction_method: memory.extraction_method,
            expires_at: memory.expires_at,
            is_active: memory.is_active,
            last_mentioned: new Date().toISOString(),
            mention_count: 1
          }, {
            onConflict: 'phone_number,memory_key',
            ignoreDuplicates: false
          })
          .select()

        if (!error && data) {
          memoryResults.push(data[0])
        }
      }

      // Auto-create semantic clusters
      await this.createSemanticClusters(phoneNumber, memoryResults)

      // Schedule verification for important memories
      await this.scheduleVerifications(phoneNumber, memoryResults)

      console.log(`[EnhancedMemory] Stored ${memories.length} enhanced memories for ${phoneNumber}`)
      return true
      
    } catch (error) {
      console.error('Enhanced memory storage error:', error)
      return false
    }
  }

  /**
   * Create semantic clusters from related memories
   */
  async createSemanticClusters(phoneNumber: string, memories: any[]): Promise<void> {
    if (!supabase) return

    // Group memories by semantic similarity
    const clusters = new Map<string, any[]>()
    
    for (const memory of memories) {
      if (memory.semantic_tags && memory.semantic_tags.length > 0) {
        const primaryTag = memory.semantic_tags[0]
        if (!clusters.has(primaryTag)) {
          clusters.set(primaryTag, [])
        }
        clusters.get(primaryTag)!.push(memory)
      }
    }

    // Create clusters with 2+ related memories
    for (const [tag, relatedMemories] of clusters) {
      if (relatedMemories.length >= 2) {
        try {
          await supabase
            .from('memory_clusters')
            .upsert({
              phone_number: phoneNumber,
              cluster_name: tag,
              cluster_type: 'semantic',
              description: `Memories related to ${tag}`,
              memory_ids: relatedMemories.map(m => m.id),
              cluster_score: Math.min(0.9, relatedMemories.length * 0.2)
            }, {
              onConflict: 'phone_number,cluster_name'
            })
        } catch (error) {
          console.error('Cluster creation error:', error)
        }
      }
    }
  }

  /**
   * Schedule verifications for important memories
   */
  async scheduleVerifications(phoneNumber: string, memories: any[]): Promise<void> {
    if (!supabase) return

    for (const memory of memories) {
      if (memory.importance_score >= 8 || memory.memory_type === 'personal') {
        try {
          await supabase
            .from('memory_verifications')
            .insert({
              memory_id: memory.id,
              phone_number: phoneNumber,
              verification_type: 'fact_check',
              verification_prompt: `Is this still accurate: ${memory.memory_summary || memory.memory_content}?`
            })
        } catch (error) {
          console.error('Verification scheduling error:', error)
        }
      }
    }
  }

  /**
   * Retrieve contextually relevant memories with clustering
   */
  async getContextualMemories(
    phoneNumber: string,
    currentContext: string,
    limit: number = 12
  ): Promise<{ memories: EnhancedMemory[], clusters: MemoryCluster[], patterns: ConversationPattern[] }> {
    if (!supabase) return { memories: [], clusters: [], patterns: [] }

    try {
      // Get active memories sorted by relevance
      const { data: memories } = await supabase
        .from('user_memories')
        .select('*')
        .eq('phone_number', phoneNumber)
        .eq('is_active', true)
        .gte('importance_score', 4)
        .order('importance_score', { ascending: false })
        .order('last_accessed', { ascending: false })
        .limit(limit)

      // Get relevant clusters
      const { data: clusters } = await supabase
        .from('memory_clusters')
        .select('*')
        .eq('phone_number', phoneNumber)
        .order('cluster_score', { ascending: false })
        .limit(5)

      // Get conversation patterns
      const { data: patterns } = await supabase
        .from('conversation_patterns')
        .select('*')
        .eq('phone_number', phoneNumber)
        .gte('confidence_level', 0.6)
        .order('confidence_level', { ascending: false })
        .limit(5)

      return {
        memories: (memories as EnhancedMemory[]) || [],
        clusters: (clusters as MemoryCluster[]) || [],
        patterns: (patterns as ConversationPattern[]) || []
      }
      
    } catch (error) {
      console.error('Contextual memory retrieval error:', error)
      return { memories: [], clusters: [], patterns: [] }
    }
  }

  /**
   * Format memories for agent context (Claude-style)
   */
  static formatEnhancedMemoriesForAgent(
    memories: EnhancedMemory[],
    clusters: MemoryCluster[],
    patterns: ConversationPattern[]
  ): string {
    if (memories.length === 0) {
      return "No previous memories about this user."
    }

    let context = "What I remember about this user:\n\n"

    // Group memories by type
    const groupedMemories = memories.reduce((acc, memory) => {
      if (!acc[memory.memory_type]) acc[memory.memory_type] = []
      acc[memory.memory_type].push(memory)
      return acc
    }, {} as Record<string, EnhancedMemory[]>)

    // Format each memory type
    Object.entries(groupedMemories).forEach(([type, mems]) => {
      context += `${type.toUpperCase()}:\n`
      mems.forEach(mem => {
        const confidence = mem.confidence_score >= 0.8 ? '' : ' (uncertain)'
        context += `• ${mem.memory_summary || mem.memory_content}${confidence}\n`
      })
      context += '\n'
    })

    // Add conversation patterns
    if (patterns.length > 0) {
      context += "COMMUNICATION PATTERNS:\n"
      patterns.forEach(pattern => {
        context += `• ${pattern.pattern_description}\n`
      })
      context += '\n'
    }

    // Add memory clusters for context
    if (clusters.length > 0) {
      context += "KEY TOPICS:\n"
      clusters.forEach(cluster => {
        if (cluster.description) {
          context += `• ${cluster.cluster_name}: ${cluster.description}\n`
        }
      })
    }

    return context
  }

  /**
   * Learn conversation patterns (cross-conversation learning)
   */
  async learnConversationPatterns(
    phoneNumber: string,
    userMessage: string,
    agentResponse: string,
    conversationHistory: Array<{ role: string, content: string }>
  ): Promise<void> {
    if (!supabase) return

    try {
      const patterns = await this.analyzeConversationPatterns(userMessage, agentResponse, conversationHistory)
      
      for (const pattern of patterns) {
        await supabase
          .from('conversation_patterns')
          .upsert({
            phone_number: phoneNumber,
            pattern_type: pattern.pattern_type,
            pattern_description: pattern.pattern_description,
            confidence_level: pattern.confidence_level,
            evidence_count: 1,
            last_observed: new Date().toISOString()
          }, {
            onConflict: 'phone_number,pattern_type,pattern_description'
          })
      }
      
    } catch (error) {
      console.error('Pattern learning error:', error)
    }
  }

  private async analyzeConversationPatterns(
    userMessage: string,
    agentResponse: string,
    conversationHistory: Array<{ role: string, content: string }>
  ): Promise<Partial<ConversationPattern>[]> {
    // Simplified pattern detection - in production, use AI analysis
    const patterns: Partial<ConversationPattern>[] = []

    // Detect communication style
    if (userMessage.length < 30 && !userMessage.includes('?')) {
      patterns.push({
        pattern_type: 'communication_style',
        pattern_description: 'prefers brief, direct messages',
        confidence_level: 0.7
      })
    }

    // Detect response preferences
    if (agentResponse.includes('hun') || agentResponse.includes('babe')) {
      patterns.push({
        pattern_type: 'response_style',
        pattern_description: 'appreciates casual, friendly tone',
        confidence_level: 0.8
      })
    }

    return patterns
  }
}